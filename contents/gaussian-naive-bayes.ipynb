{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72976d2-9387-470b-9426-13471507a689",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "This note introduces the **Gaussian Naive Bayes** algorithm using `scikit‑learn`, explains the step‑by‑step logic behind how it works, and then demonstrates a from‑scratch implementation to show that the core idea is simple and easy to build.\n",
    "\n",
    "## What is Gaussian Naive Bayes?\n",
    "\n",
    "Gaussian Naive Bayes is a classifier designed for **continuous numerical features**, such as height, weight, or petal length.\n",
    "\n",
    "Instead of counting how often a word appears (like in text classification), it assumes that each feature follows a **normal distribution** for each class.\n",
    "\n",
    "For example, if we want to classify flowers as `Setosa` or `Versicolor` based on petal width, Gaussian NB models how petal widths are distributed **within each class**, learning:\n",
    "\n",
    "- The **mean** and **variance** of each feature for each class\n",
    "- Then uses the **Gaussian probability density function** to score how likely a new value is under each class\n",
    "\n",
    "After summing all those log-probabilities and adding class priors, the class with the higher total wins.\n",
    "\n",
    "It learns these statistics from past data — how features are distributed in each class. This makes the model well-suited for real‑valued, continuous input data.\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "- **Use `scikit‑learn`** to demonstrate how Gaussian Naive Bayes works in practice  \n",
    "- **Explain the logic behind it** in an intuitive way (mean/variance estimation, Gaussian formula, using logs for numerical stability)  \n",
    "- **Show how to implement the same idea step by step from scratch**  \n",
    "\n",
    "Let’s dive into the details to understand how it works and how to implement it ourselves.\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88abf321-478a-4ae0-9b60-6c49ade10c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 samples × 4 features\n",
      "Classes: ['setosa', 'versicolor', 'virginica']\n",
      "\n",
      "Sample feature values:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 0. Imports\n",
    "# --------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load the dataset\n",
    "# --------------------------------------------------\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Loaded {X.shape[0]} samples × {X.shape[1]} features\")\n",
    "print(\"Classes:\", list(target_names))\n",
    "print(\"\\nSample feature values:\")\n",
    "print(X.head())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Train-test split\n",
    "# --------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b2d68-bc53-4ebf-86b4-d6cbe3c1b12e",
   "metadata": {},
   "source": [
    "## Data Observation\n",
    "\n",
    "These real-valued features describe physical measurements of iris flowers. Since the features are continuous and follow roughly Gaussian distributions per class, this dataset is ideal for **Gaussian Naive Bayes**.\n",
    "\n",
    "## Implement with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2878c4c-01cc-4cec-87d8-3d4b90c9ba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy = 0.9211\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 3. Train Gaussian Naive Bayes\n",
    "# --------------------------------------------------\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Predict and evaluate\n",
    "# --------------------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Gaussian Naive Bayes accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa834f-e102-4cd5-a877-d0c2e7051344",
   "metadata": {},
   "source": [
    "## Behind the Scenes: Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes is a way to classify data by looking at **real-valued features** and modeling them with **normal (Gaussian) distributions**.\n",
    "\n",
    "Unlike Bernoulli Naive Bayes (which works on binary features) or Multinomial Naive Bayes (which uses word counts), Gaussian Naive Bayes assumes that **each feature is continuous** and normally distributed within each class.\n",
    "\n",
    "### Bayes' Theorem for Classification\n",
    "\n",
    "We want to know:\n",
    "\n",
    "> “What is the probability this input belongs to a class, given its feature values?”\n",
    "\n",
    "We write this as:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid \\text{features}) \\propto P(\\text{features} \\mid \\text{class}) \\cdot P(\\text{class})\n",
    "$$\n",
    "\n",
    "We calculate this score for every class and choose the one with the highest value.\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "We assume each feature is conditionally independent given the class:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, ..., x_n \\mid \\text{class}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{class})\n",
    "$$\n",
    "\n",
    "### Input Format\n",
    "\n",
    "Each input is a vector of continuous values like:\n",
    "\n",
    "- `[5.1, 3.5, 1.4, 0.2]` → sepal/petal measurements of a flower\n",
    "\n",
    "This is why we write:\n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Which means each feature $x_i$ is a **real number**.\n",
    "\n",
    "### Estimating Feature Probabilities from Training Data\n",
    "\n",
    "To calculate $P(x_i \\mid \\text{class})$, we assume each feature follows a **Gaussian distribution** for each class.\n",
    "\n",
    "We look at **all training samples in a class** and calculate:\n",
    "\n",
    "- $\\mu$ = the **mean** of feature $x_i$ in this class  \n",
    "- $\\sigma^2$ = the **variance** of feature $x_i$ in this class\n",
    "\n",
    "Then we use the **Gaussian PDF**:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{class}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "> This tells us:  \n",
    "> “If we randomly pick a value of feature $x_i$ from this class, how likely is it to be near the value we just observed?”\n",
    "\n",
    "In other words, it’s the **likelihood** of seeing $x_i$ under the bell curve for that class.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Imagine we are training a classifier for two classes: `pass` and `fail`, based on one feature: `study_hours`.\n",
    "\n",
    "We analyze the training data and compute:\n",
    "\n",
    "- For `pass`:  \n",
    "  - $\\mu = 8$, $\\sigma^2 = 1$  \n",
    "- For `fail`:  \n",
    "  - $\\mu = 4$, $\\sigma^2 = 1$\n",
    "\n",
    "Now for a new sample with `study_hours = 6`, we compute:\n",
    "\n",
    "- For `pass`:\n",
    "\n",
    "$$\n",
    "P(6 \\mid \\text{pass}) = \\frac{1}{\\sqrt{2\\pi \\cdot 1}} \\exp\\left( - \\frac{(6 - 8)^2}{2 \\cdot 1} \\right)  \n",
    "= \\frac{1}{\\sqrt{2\\pi}} \\exp(-2) \\approx 0.05399\n",
    "$$\n",
    "\n",
    "- For `fail`:\n",
    "\n",
    "$$\n",
    "P(6 \\mid \\text{fail}) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( - \\frac{(6 - 4)^2}{2} \\right)  \n",
    "= \\frac{1}{\\sqrt{2\\pi}} \\exp(-2) \\approx 0.05399\n",
    "$$\n",
    "\n",
    "> This tells the model:\n",
    "> - `6` is equally likely under both curves  \n",
    "> - So the final prediction would depend on the **class priors**\n",
    "\n",
    "### Why Use a Gaussian?\n",
    "\n",
    "In real-world data, many measurements (like height, weight, test scores, petal length, etc.) naturally follow a **bell-shaped curve** — they tend to cluster around an average value, with fewer very small or very large cases.  \n",
    "\n",
    "This is called a **normal distribution** (or Gaussian), and it’s a good fit for many features in real datasets.\n",
    "\n",
    "The **Gaussian PDF** gives us a way to **score how typical** a value is.  \n",
    "It forms the foundation of Gaussian Naive Bayes — converting raw values into likelihoods, which are used for classification.\n",
    "\n",
    "> This is why Gaussian Naive Bayes works well on numerical, continuous data.\n",
    "\n",
    "### Combine with Prior Probability\n",
    "\n",
    "We also multiply by the prior probability of each class:\n",
    "\n",
    "$$\n",
    "P(\\text{class}) = \\frac{\\text{Number of training samples in this class}}{\\text{Total number of training samples}}\n",
    "$$\n",
    "\n",
    "So the full class score becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid X) \\propto P(\\text{class}) \\cdot \\prod_{i=1}^{n} P(x_i \\mid \\text{class})\n",
    "$$\n",
    "\n",
    "> 🔄 This is where **Gaussian Naive Bayes** differs from **Bernoulli and Multinomial**:\n",
    "\n",
    "- **BernoulliNB**: binary word presence (1 or 0)  \n",
    "- **MultinomialNB**: word counts (integers ≥ 0)  \n",
    "- **GaussianNB**: continuous real-valued features, modeled using normal distributions\n",
    "\n",
    "> 🧠 Intuition:\n",
    "\n",
    "- GaussianNB assumes each feature in each class forms a **bell-shaped curve**\n",
    "- It uses the curve to measure how \"typical\" a feature value is for that class\n",
    "\n",
    "The more typical the values are for a class, the higher the score.\n",
    "\n",
    "### Final Scoring Formula (with Logs)\n",
    "\n",
    "To avoid tiny numbers and numerical instability, we move everything into **log space**.\n",
    "\n",
    "We want to compute:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid X) \\propto P(\\text{class}) \\cdot \\prod_{i=1}^{n} P(x_i \\mid \\text{class})\n",
    "$$\n",
    "\n",
    "Taking the logarithm turns the product into a sum:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{class} \\mid X) \\propto \\log P(\\text{class}) + \\sum_{i=1}^{n} \\log P(x_i \\mid \\text{class})\n",
    "$$\n",
    "\n",
    "#### Step-by-step: Log of the Gaussian PDF\n",
    "\n",
    "Recall the Gaussian PDF for a single feature $x_i$:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{class}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left( - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Taking the log of both sides:\n",
    "\n",
    "$$\n",
    "\\log P(x_i \\mid \\text{class}) \n",
    "= \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\right) \n",
    "+ \\log \\left( \\exp\\left( - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right) \\right)\n",
    "$$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$$\n",
    "\\log P(x_i \\mid \\text{class}) = \n",
    "- \\frac{1}{2} \\log(2\\pi\\sigma^2) \n",
    "- \\frac{(x_i - \\mu)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "This gives us the complete formula for each feature’s log-probability.\n",
    "\n",
    "### Final Decision Rule\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{class} \\mid X) \\propto \n",
    "\\log P(\\text{class}) \n",
    "+ \\sum_{i=1}^{n} \\left[ - \\frac{1}{2} \\log(2\\pi\\sigma^2) \n",
    "- \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "This score is computed for **each class**, and the model selects the one with the **highest log-probability**.\n",
    "\n",
    "## Let's Code It\n",
    "\n",
    "Now that we understand how it works, let’s implement it from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afff5d6-bb0d-4265-85b4-3e1e30849ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch accuracy = 0.9211\n"
     ]
    }
   ],
   "source": [
    "class MyGaussianNB:\n",
    "    def __init__(self):\n",
    "        # No smoothing hyperparameter needed.\n",
    "        # Gaussian Naive Bayes assumes each feature follows:\n",
    "        # P(x_i | class) = (1 / sqrt(2πσ²)) * exp( - (x_i - μ)² / (2σ²) )\n",
    "        pass\n",
    "\n",
    "    # ==================== TRAIN ====================\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        X  ─ shape (n_samples, n_features)\n",
    "             Each row is a sample, each column is a real-valued feature.\n",
    "        y  ─ shape (n_samples,)\n",
    "             Each value is the class label (e.g. 0, 1, 2)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # ---------- 1. PRIOR ----------\n",
    "        # P(class) = (# samples in class) / (total samples)\n",
    "        # log_prior[c] = log P(class = c)\n",
    "        class_counts = np.bincount(y, minlength=n_classes)\n",
    "        self.log_prior_ = np.log(class_counts / n_samples)  # shape (n_classes,)\n",
    "\n",
    "        # ---------- 2. MEAN & VARIANCE ----------\n",
    "        # For each class c:\n",
    "        # μ_c = mean of feature x_i in class c\n",
    "        # σ²_c = variance of feature x_i in class c\n",
    "        self.mean_ = np.zeros((n_classes, n_features))\n",
    "        self.var_  = np.zeros((n_classes, n_features))\n",
    "\n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]                      # Get samples in class c\n",
    "            self.mean_[c, :] = X_c.mean(axis=0)  # μ_c\n",
    "            self.var_[c, :]  = X_c.var(axis=0)   # σ²_c\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ==================== GAUSSIAN LOG-LIKELIHOOD ====================\n",
    "    def _log_likelihood(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Compute log P(x | class) for each class using:\n",
    "\n",
    "        log P(x_i | class) = -0.5 * log(2πσ²) - ((x_i - μ)² / (2σ²))\n",
    "\n",
    "        Final likelihood for a sample x:\n",
    "        log P(x | class) = sum over all features i of log P(x_i | class)\n",
    "        \"\"\"\n",
    "        eps = 1e-9  # avoid division by zero\n",
    "        num = -0.5 * ((x - self.mean_) ** 2) / (self.var_ + eps)  # squared deviation\n",
    "        log_pdf = -0.5 * np.log(2 * np.pi * self.var_ + eps) + num  # full log-PDF\n",
    "        return log_pdf.sum(axis=1)  # total log-likelihood across features\n",
    "\n",
    "    # =================== PREDICT ===================\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        For each input sample x:\n",
    "        Compute the class score:\n",
    "\n",
    "        log P(class | x) ∝ log P(class) + log P(x | class)\n",
    "\n",
    "        Then choose the class with the highest total log-probability.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            log_likelihood = self._log_likelihood(x)     # log P(x | class)\n",
    "            log_posterior = self.log_prior_ + log_likelihood  # total score\n",
    "            best_class = self.classes_[np.argmax(log_posterior)]\n",
    "            predictions.append(best_class)\n",
    "        return np.array(predictions)\n",
    "\n",
    "my_nb = MyGaussianNB().fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred_my = my_nb.predict(X_test.to_numpy())\n",
    "\n",
    "acc_my = accuracy_score(y_test, y_pred_my)\n",
    "print(f\"scratch accuracy = {acc_my:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ed8d6-a778-4b14-9d2c-0311fcb14130",
   "metadata": {},
   "source": [
    "## It Works!\n",
    "\n",
    "The scratch model hits an accuracy of **0.9211**, matching `scikit-learn`.\n",
    "\n",
    "This confirms that the logic behind **Gaussian Naive Bayes** — estimating class‑conditional means and variances, applying the Gaussian PDF, summing log-scores, and picking the class with the highest total — behaves exactly as expected.\n",
    "\n",
    "We’ve successfully built **Gaussian Naive Bayes** from the ground up!"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-21T05:06:42.966422+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-21T05:06:42.966422+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
