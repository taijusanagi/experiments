{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4405c3-df1d-4e26-8b1d-5429ff91c955",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This note introduces the **Linear Regression** algorithm using `scikit-learn`, explains the step-by-step logic behind how it works, and then demonstrates a from-scratch implementation to show that the core idea is simple and easy to build.\n",
    "\n",
    "## What is Linear Regression?\n",
    "\n",
    "Linear Regression is like drawing the best straight line through a set of points.\n",
    "\n",
    "The line represents a relationship between the **input feature** and the **predicted value** — like how a person's weight might relate to their height.\n",
    "\n",
    "It learns from existing data to find the \"best fit line\" and uses it to make predictions on new data.\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "- Use `scikit-learn` to demonstrate how Linear Regression works in practice  \n",
    "- Explain the logic behind it in an intuitive way  \n",
    "- Show how to implement the same idea step by step from scratch  \n",
    "\n",
    "Let’s dive into the details to understand how it works and how to implement it ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578b07f-02f8-4111-b17a-4f5a1e56eaa0",
   "metadata": {},
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "### 1. The Goal\n",
    "\n",
    "We want to find the best-fitting straight line:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ is the predicted value  \n",
    "- $x$ is the input  \n",
    "- $w$ is the slope (how steep the line is)  \n",
    "- $b$ is the intercept (where it crosses the y-axis)\n",
    "\n",
    "### 2. How Good is the Line? (Cost Function)\n",
    "\n",
    "To measure how well the line fits the data, we use **Mean Squared Error (MSE)** as our **cost function**:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2\n",
    "= \\frac{1}{2n} \\sum_{i=1}^{n} \\left( w x_i + b - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- For each data point, we compute the **difference between predicted and actual value**: $(\\hat{y}_i - y_i)$\n",
    "- Then we **square the difference** to:\n",
    "  - Make all errors positive (cancel out minus and plus)\n",
    "  - Make the model more sensitive to **larger errors**\n",
    "- Finally, we average the squared errors across all $n$ samples\n",
    "\n",
    "We use $\\frac{1}{2n}$ instead of $\\frac{1}{n}$ to simplify the math when we take derivatives later.\n",
    "\n",
    "### 3. How to Minimize the Cost? (Gradient Descent)\n",
    "\n",
    "We want to adjust the values of $w$ and $b$ to make the cost function $J(w, b)$ as small as possible.\n",
    "\n",
    "To do this, we use an algorithm called **gradient descent**, which:\n",
    "- Calculates the slope of the cost function\n",
    "- Takes small steps in the direction that reduces the cost\n",
    "- Repeats this process **over and over again** — adjusting $w$ and $b$ a little each time\n",
    "\n",
    "It keeps checking:\n",
    "> \"Which direction should I move to make the cost smaller?\"\n",
    "\n",
    "To move in that direction, we compute the partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} \\quad \\text{and} \\quad \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "And update like this:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "> $\\alpha$ is the **learning rate** — a small number that controls how big each update step is\n",
    "\n",
    "This cycle of \"calculate slope → move → calculate again\" continues until the model improves and the cost becomes low enough.\n",
    "\n",
    "Before diving into the derivative formulas, let’s quickly review two key tools that make it all work.\n",
    "\n",
    "### 4. A Quick Review: Power Rule and Chain Rule\n",
    "\n",
    "#### Power Rule:\n",
    "\n",
    "If a function contains a square like:\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n",
    "\n",
    "Then the derivative is:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} f(x) = 2x\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "If your function contains something squared, its derivative will be **2 times that thing**.\n",
    "\n",
    "#### Chain Rule:\n",
    "\n",
    "Now, what if the thing being squared is itself a function?  \n",
    "Let’s say:\n",
    "\n",
    "$$\n",
    "f(x) = [g(x)]^2\n",
    "$$\n",
    "\n",
    "Then the chain rule says:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} f(x) = 2 \\cdot g(x) \\cdot g'(x)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- You treat the whole inner function $g(x)$ like a single variable and apply the power rule: $2 \\cdot g(x)$\n",
    "- Then multiply by the **derivative of the inside part**: $g'(x)$\n",
    "\n",
    "So:  \n",
    "\n",
    "**Chain rule = outer derivative × inner derivative**\n",
    "\n",
    "### 5. Derivatives Step-by-Step (Putting It All Together)\n",
    "\n",
    "Now we compute $\\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$ **separately**, starting from a single data point to keep things simple.\n",
    "\n",
    "Let’s say we have just one data point:\n",
    "\n",
    "- Input: $x$\n",
    "- Actual output: $y$\n",
    "- Predicted output: $\\hat{y} = wx + b$\n",
    "\n",
    "So the cost for this single point is:\n",
    "\n",
    "$$\n",
    "J = (wx + b - y)^2\n",
    "$$\n",
    "\n",
    "We define the **inner function**:\n",
    "\n",
    "$$\n",
    "u = wx + b - y\n",
    "$$\n",
    "\n",
    "So the cost becomes:\n",
    "\n",
    "$$\n",
    "J = u^2\n",
    "$$\n",
    "\n",
    "This prepares us to apply the **chain rule** to $J = u^2$.\n",
    "\n",
    "#### Derivative with respect to \\( w \\)\n",
    "\n",
    "Step-by-step:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dw} = \\frac{d}{dw} (u^2)\n",
    "$$\n",
    "\n",
    "Now apply the chain rule:\n",
    "\n",
    "- Outer derivative: $\\frac{d}{du}(u^2) = 2u$\n",
    "- Inner derivative:\n",
    "  - $\\frac{du}{dw} = x$, because when we change $w$, only the $wx$ term affects the slope. $x$ is constant, so the slope is $x$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dw} = 2u \\cdot x = 2(wx + b - y) \\cdot x\n",
    "$$\n",
    "\n",
    "#### Derivative with respect to \\( b \\)\n",
    "\n",
    "Same logic:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{db} = \\frac{d}{db} (u^2)\n",
    "$$\n",
    "\n",
    "Apply the chain rule:\n",
    "\n",
    "- Outer derivative: $\\frac{d}{du}(u^2) = 2u$\n",
    "- Inner derivative:\n",
    "  - $\\frac{du}{db} = 1$, because when we change $b$, it directly adds to $u$. The slope is 1, and other parts don’t change\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{db} = 2(wx + b - y)\n",
    "$$\n",
    "\n",
    "### 6. Generalizing to All Data Points\n",
    "\n",
    "Now that we’ve understood the derivatives for a single data point, we can scale this up to the whole dataset.\n",
    "\n",
    "We repeat the same process for each training example $(x_i, y_i)$ and sum the gradients.  \n",
    "Our full cost function over all points is:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( w x_i + b - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "#### Derivative with respect to \\( w \\)\n",
    "\n",
    "Apply the chain rule:\n",
    "\n",
    "- Outer derivative of the square: $2(w x_i + b - y_i)$\n",
    "- Inner derivative with respect to $w$: $x_i$\n",
    "\n",
    "So the full gradient becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "= \\frac{1}{2n} \\sum_{i=1}^{n} 2(w x_i + b - y_i) \\cdot x_i\n",
    "$$\n",
    "\n",
    "Now cancel the 2s:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} (w x_i + b - y_i) \\cdot x_i\n",
    "$$\n",
    "\n",
    "#### Derivative with respect to \\( b \\)\n",
    "\n",
    "Same idea:\n",
    "\n",
    "- Outer derivative: $2(w x_i + b - y_i)$\n",
    "- Inner derivative with respect to $b$: $1$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "= \\frac{1}{2n} \\sum_{i=1}^{n} 2(w x_i + b - y_i)\n",
    "$$\n",
    "\n",
    "Cancel the 2s:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} (w x_i + b - y_i)\n",
    "$$\n",
    "\n",
    "### 7. When to Stop: Convergence Criteria\n",
    "\n",
    "Gradient descent doesn’t just make one update — it **keeps looping**:\n",
    "\n",
    "1. Calculate the gradient (slope) using the current values of $w$ and $b$\n",
    "2. Use the learning rate $\\alpha$ to take a step downhill\n",
    "3. Repeat: update $w$, update $b$, and calculate again\n",
    "\n",
    "This cycle continues until one of the following happens:\n",
    "\n",
    "- **Convergence**: The cost becomes very small, and further updates barely change the result. For example, if the difference in cost between steps is less than a small number like $10^{-6}$, we say it’s close enough.\n",
    "\n",
    "- **Maximum iterations**: We set a safety limit (e.g. 1000 steps) so it won’t run forever.\n",
    "\n",
    "- **No significant improvement**: If the cost hasn’t improved for many steps, we might be stuck in a flat spot or local minimum. In that case, it’s better to stop early.\n",
    "\n",
    "> So gradient descent is like a climber constantly feeling the slope, moving slowly downhill — and stopping when it’s either flat enough, time’s up, or there's no more progress.\n",
    "\n",
    "This completes the core learning process — now we’re ready to visualize or implement it in code!\n"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-18T00:14:04.305798+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-18T00:14:04.305798+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
