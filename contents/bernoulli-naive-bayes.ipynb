{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253f824e-fa49-420e-8124-d01cb796a8f7",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "## Probability Basics\n",
    "\n",
    "Probability tells us how likely something is to happen. The result is always between 0 and 1.\n",
    "\n",
    "In this section, we’ll learn 3 essential concepts:\n",
    "\n",
    "1. Marginal Probability  \n",
    "2. Joint Probability  \n",
    "3. The Product Rule (with Conditional Probability)\n",
    "\n",
    "We’ll use **coin flips** for all examples.\n",
    "\n",
    "### 1. Marginal Probability\n",
    "\n",
    "This is the probability of a single event happening.\n",
    "\n",
    "**Example — Flipping a coin:**\n",
    "\n",
    "- The probability of getting Heads:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "- The probability of getting Tails:\n",
    "\n",
    "$$\n",
    "P(\\text{Tails}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "These are called **marginal probabilities** — they only involve one event.\n",
    "\n",
    "### 2. Joint Probability\n",
    "\n",
    "This is the probability of **two events happening together**:\n",
    "\n",
    "$$\n",
    "P(A \\cap B)\n",
    "$$\n",
    "\n",
    "**Example — Flipping two coins:**\n",
    "\n",
    "- What’s the probability of getting Heads on the first **and** Heads on the second?\n",
    "\n",
    "If the flips are **independent**:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads on both}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "So for **independent events only**, the joint probability is:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "If the events are **not independent**, we need the **product rule**.\n",
    "\n",
    "### 3. The Product Rule (with Conditional Probability)\n",
    "\n",
    "The **product rule** handles both independent and dependent events:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A \\mid B) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "This introduces **conditional probability**:\n",
    "\n",
    "- $P(A \\mid B)$ means \"the probability of $A$, assuming $B$ has already happened.\"\n",
    "\n",
    "**Example — Dependent coin flips:**\n",
    "\n",
    "- $P(\\text{First = Heads}) = \\frac{1}{2}$\n",
    "- If the first flip is Heads, we use a **special biased coin** for the second flip:  \n",
    "  $P(\\text{Second = Heads} \\mid \\text{First = Heads}) = 0.75$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(\\text{Both Heads}) = \\frac{1}{2} \\cdot 0.75 = 0.375\n",
    "$$\n",
    "\n",
    "This shows how a conditional bias affects the joint probability.\n",
    "\n",
    "### From Product Rule to Bayes’ Theorem\n",
    "\n",
    "We can also write the product rule the other way:\n",
    "\n",
    "$$\n",
    "P(B \\cap A) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Since $P(A \\cap B) = P(B \\cap A)$, we can set them equal:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Now divide both sides by $P(B)$:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This is **Bayes’ Theorem** — a way to reverse the direction of conditional probability.\n",
    "\n",
    "## Bayes' Theorem for Classification\n",
    "\n",
    "Now let:\n",
    "- $A = \\text{Spam}$  \n",
    "- $B = \\text{Words in the email}$\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) = \\frac{P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Words})}\n",
    "$$\n",
    "\n",
    "Then: \n",
    "\n",
    "- We know how often certain words appear in spam:  \n",
    "  → $P(\\text{Words} \\mid \\text{Spam})$\n",
    "\n",
    "- We know the overall proportion of spam:  \n",
    "  → $P(\\text{Spam})$\n",
    "\n",
    "Bayes’ Theorem helps us compute $P(\\text{Spam} \\mid \\text{Words})$ — the chance an email is spam **given** the words it contains.\n",
    "\n",
    "We usually compare two classes:\n",
    "\n",
    "$$\n",
    "\\frac{P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Words})}\n",
    "\\quad \\text{vs} \\quad\n",
    "\\frac{P(\\text{Words} \\mid \\text{Not Spam}) \\cdot P(\\text{Not Spam})}{P(\\text{Words})}\n",
    "$$\n",
    "\n",
    "Since the denominator is the same, it cancels out:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})\n",
    "$$\n",
    "\n",
    "> The symbol $\\propto$ means \"proportional to\" — we're only interested in which class has the higher score, so we can safely ignore the shared denominator.\n",
    "\n",
    "## The Naive Assumption\n",
    "\n",
    "Just like assuming each coin flip is independent makes probability easier to compute, we assume all words are **conditionally independent** given the class — so the overall probability can be calculated by simply multiplying each word’s probability:\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam}) = P(x_1 \\mid \\text{Spam}) \\cdot P(x_2 \\mid \\text{Spam}) \\cdot \\cdots \\cdot P(x_n \\mid \\text{Spam}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{Spam})\n",
    "$$\n",
    "\n",
    "> The symbol $\\prod$ is the product symbol — like $\\sum$ but for multiplication.\n",
    "\n",
    "It's called *Naive* because this independence assumption is rarely true in real language — but it works surprisingly well in practice.\n",
    "\n",
    "## Calculating Word Probabilities in Spam Emails\n",
    "\n",
    "Now that we assume each word is independent, we need a way to calculate how likely each word appears in spam emails.\n",
    "\n",
    "Here’s how it works:\n",
    "\n",
    "### 1. Learn Word Probabilities from Training Data\n",
    "\n",
    "From the training data, we count how often each word appears in spam emails.\n",
    "\n",
    "Let’s define:\n",
    "\n",
    "- $x_i = 1$ means the $i$-th word is **present** in the email  \n",
    "- $x_i = 0$ means the $i$-th word is **absent**\n",
    "\n",
    "We want to estimate:\n",
    "\n",
    "$$\n",
    "\\theta_i = P(x_i = 1 \\mid \\text{Spam}) = \\frac{\\text{Number of spam emails containing word } i}{\\text{Total number of spam emails}}\n",
    "$$\n",
    "\n",
    "This gives us the probability that a spam email **contains** word $i$ at least once.\n",
    "\n",
    "#### To Avoid Zero Probability: Laplace Smoothing\n",
    "\n",
    "Sometimes a word might never appear in spam emails, which would give us a probability of 0 and cause the whole product to collapse.\n",
    "\n",
    "To fix that, we use **Laplace smoothing**:\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{\\text{Number of spam emails containing word } i + \\alpha}{\\text{Total number of spam emails} + 2\\alpha}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is a small constant (usually 1)\n",
    "- $2\\alpha$ accounts for both word **present** and **absent** cases\n",
    "\n",
    "### 2. Check the Email for Each Word\n",
    "\n",
    "For a new email, we check each word $x_i$:\n",
    "\n",
    "- If the word **is present**: we use $\\theta_i$  \n",
    "- If the word **is absent**: we use $1 - \\theta_i$\n",
    "\n",
    "We can express both in a single formula:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{Spam}) = \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Why it works:\n",
    "\n",
    "- If $x_i = 1$:  \n",
    "  $P(x_i \\mid \\text{Spam}) = \\theta_i$\n",
    "\n",
    "- If $x_i = 0$:  \n",
    "  $P(x_i \\mid \\text{Spam}) = 1 - \\theta_i$\n",
    "\n",
    "### 3. Multiply All Word Probabilities Together\n",
    "\n",
    "Since we assumed all words are conditionally independent:\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam}) = \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "So we multiply each word’s probability — whether it’s present or not — to get the full likelihood for the email.\n",
    "\n",
    "This is how Bernoulli Naive Bayes models text classification.\n",
    "\n",
    "### 4. Multiply All Word Probabilities Together\n",
    "\n",
    "Assuming independence:\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam}) = \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "### 5. Combine with Prior\n",
    "\n",
    "We also multiply by the prior probability of spam:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam}) = \\frac{\\text{Number of spam emails}}{\\text{Total number of emails}}\n",
    "$$\n",
    "\n",
    "So the full expression becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Spam}) \\cdot \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "## But There’s a Problem: Underflow\n",
    "\n",
    "Multiplying many small probabilities can make the result **extremely tiny** — too small for the computer to represent accurately.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "0.9 \\cdot 0.6 \\cdot 0.1 \\cdot 0.02 = 0.00108\n",
    "$$\n",
    "\n",
    "With hundreds of terms, the value might become so small it gets rounded to **0**. This is called **underflow**, and it breaks our calculation.\n",
    "\n",
    "### The Fix: Use Logarithms\n",
    "\n",
    "Instead of multiplying probabilities, we take the **log** and add them — this avoids underflow and is easier to compute.\n",
    "\n",
    "### Log Rules We’ll Use\n",
    "\n",
    "We’ll use these two key log rules:\n",
    "\n",
    "- $\\log(a \\cdot b) = \\log a + \\log b$  \n",
    "- $\\log(a^b) = b \\cdot \\log a$  \n",
    "\n",
    "### Why It Works: A Simple Example\n",
    "\n",
    "Let’s take the log of the earlier multiplication:\n",
    "\n",
    "$$\n",
    "\\log(0.9 \\cdot 0.6 \\cdot 0.1 \\cdot 0.02) = \\log(0.9) + \\log(0.6) + \\log(0.1) + \\log(0.02)\n",
    "$$\n",
    "\n",
    "Approximate values:\n",
    "\n",
    "$$\n",
    "= -0.105 + (-0.222) + (-1.0) + (-1.699) = -3.026\n",
    "$$\n",
    "\n",
    "Instead of a tiny number like **0.00108**, we now get a manageable value: **–3.026**\n",
    "\n",
    "### Apply to Bernoulli Naive Bayes\n",
    "\n",
    "We start from:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Spam}) \\cdot \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Take the log:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\log \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Convert the product into a sum:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\log\\left( \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i} \\right)\n",
    "$$\n",
    "\n",
    "Break inside the log:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\left[ \\log(\\theta_i^{x_i}) + \\log((1 - \\theta_i)^{1 - x_i}) \\right]\n",
    "$$\n",
    "\n",
    "Apply the power rule:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\left[ x_i \\log \\theta_i + (1 - x_i) \\log(1 - \\theta_i) \\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-18T05:26:02.906712+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-18T08:44:43.846124+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
