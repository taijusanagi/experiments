{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02d72ee-2e9f-4c86-845f-d98b90a9ba1b",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "This note introduces the **Bernoulli Naive Bayes** algorithm using `scikit‑learn`, explains the step‑by‑step logic behind how it works, and then demonstrates a from‑scratch implementation to show that the core idea is simple and easy to build.\n",
    "\n",
    "## What is Bernoulli Naive Bayes?\n",
    "\n",
    "Bernoulli Naive Bayes is like keeping a **yes / no checklist** for every e‑mail, for example:\n",
    "\n",
    "- Does the message contain the word *money*?\n",
    "\n",
    "This single‑word question is just an example — in practice the model asks the same *yes / no* question for **every word in the vocabulary**, and each word’s answer adds its own little score toward **spam** or **ham**. \n",
    "\n",
    "After summing all those scores, the class with the higher total wins.\n",
    "\n",
    "It learns these scores from past data (how often each word shows up in spam versus ham). Adding them word‑by‑word is fast, requires no tuning knobs, and often beats fancier models on text tasks with binary features.\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "- **Use `scikit‑learn`** to demonstrate how Bernoulli Naive Bayes works in practice  \n",
    "- **Explain the logic behind it** in an intuitive way (scorecard view, Laplace smoothing, using logs instead of tiny products)  \n",
    "- **Show how to implement the same idea step by step from scratch**  \n",
    "\n",
    "Let’s dive into the details to understand how it works and how to implement it ourselves.\n",
    "\n",
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ab09b-072b-4345-ad4b-4f0ee5d1f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4601 emails × 57 features\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes on a REAL spam / ham dataset\n",
    "# ==================================================\n",
    "# “Spambase” (UCI ML Repo, OpenML ID = 44)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 0. Imports\n",
    "# --------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load & binarise the dataset                 (↔ Section “Learn Word Probabilities”)\n",
    "# --------------------------------------------------\n",
    "spam = fetch_openml(\"spambase\", version=1, as_frame=False)   # ~4 KB download\n",
    "X_raw, y = spam.data, spam.target.astype(int)                # y ∈ {0 = ham, 1 = spam}\n",
    "\n",
    "# Bernoulli NB expects 0/1 “word present?” flags.\n",
    "X = Binarizer(threshold=0.0).fit_transform(X_raw)            # >0 → 1, else 0\n",
    "\n",
    "# Keep a test split for honest evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Loaded {X.shape[0]} emails × {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544af9a9-ba60-4983-8b90-c838e824a3a5",
   "metadata": {},
   "source": [
    "## Data Observation\n",
    "\n",
    "`X` holds 57 binary word‑flags (e.g. `word_freq_money = 1` if **money** appears); `y` is the spam / ham label for each e‑mail.  \n",
    "By matching these flags to the labels, Bernoulli Naive Bayes learns a separate spam‑vs‑ham score for each word.\n",
    "\n",
    "## Implement with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8aa38-a063-4017-abc8-ab8905482987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit‑learn accuracy = 0.8818\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 2. Reference model: scikit‑learn BernoulliNB\n",
    "# --------------------------------------------------\n",
    "sk_model = BernoulliNB(alpha=1.0)            # Laplace α = 1  (Section ‘Laplace Smoothing’)\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk  = sk_model.predict(X_test)\n",
    "y_proba_sk = sk_model.predict_proba(X_test)[:, 1]            # P(spam)\n",
    "acc_sk     = accuracy_score(y_test, y_pred_sk)\n",
    "print(f\"scikit‑learn accuracy = {acc_sk:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f824e-fa49-420e-8124-d01cb796a8f7",
   "metadata": {},
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "Before we dive into the model, let’s understand the foundation it relies on: probability. This will give us the tools to calculate how likely an email is spam based on the words it contains.\n",
    "\n",
    "### Probability Basics\n",
    "\n",
    "Probability tells us how likely something is to happen. The result is always between 0 and 1.\n",
    "\n",
    "In this section, we’ll learn 3 essential concepts:\n",
    "\n",
    "1. Marginal Probability  \n",
    "2. Joint Probability  \n",
    "3. The Product Rule (with Conditional Probability)\n",
    "\n",
    "We’ll use **coin flips** for all examples.\n",
    "\n",
    "#### 1. Marginal Probability\n",
    "\n",
    "This is the probability of a single event happening.\n",
    "\n",
    "**Example — Flipping a coin:**\n",
    "\n",
    "- The probability of getting Heads:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "- The probability of getting Tails:\n",
    "\n",
    "$$\n",
    "P(\\text{Tails}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "These are called **marginal probabilities** — they only involve one event.\n",
    "\n",
    "#### 2. Joint Probability\n",
    "\n",
    "This is the probability of **two events happening together**:\n",
    "\n",
    "$$\n",
    "P(A \\cap B)\n",
    "$$\n",
    "\n",
    "**Example — Flipping two coins:**\n",
    "\n",
    "- What’s the probability of getting Heads on the first **and** Heads on the second?\n",
    "\n",
    "If the flips are **independent**:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads on both}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "So for **independent events only**, the joint probability is:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "If the events are **not independent**, we need the **product rule**.\n",
    "\n",
    "#### 3. The Product Rule (with Conditional Probability)\n",
    "\n",
    "The **product rule** handles both independent and dependent events:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A \\mid B) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "This introduces **conditional probability**:\n",
    "\n",
    "- $P(A \\mid B)$ means \"the probability of $A$, assuming $B$ has already happened.\"\n",
    "\n",
    "**Example — Dependent coin flips:**\n",
    "\n",
    "- $P(\\text{First = Heads}) = \\frac{1}{2}$\n",
    "- If the first flip is Heads, we use a **special biased coin** for the second flip:  \n",
    "  $P(\\text{Second = Heads} \\mid \\text{First = Heads}) = 0.75$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(\\text{Both Heads}) = \\frac{1}{2} \\cdot 0.75 = 0.375\n",
    "$$\n",
    "\n",
    "This shows how a conditional bias affects the joint probability.\n",
    "\n",
    "#### From Product Rule to Bayes’ Theorem\n",
    "\n",
    "We can also write the product rule the other way:\n",
    "\n",
    "$$\n",
    "P(B \\cap A) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Since $P(A \\cap B) = P(B \\cap A)$, we can set them equal:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Now divide both sides by $P(B)$:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This is **Bayes’ Theorem** — a way to reverse the direction of conditional probability.\n",
    "\n",
    "Let’s see how we apply this to spam classification.\n",
    "\n",
    "### Bayes' Theorem for Classification\n",
    "\n",
    "Now let:\n",
    "- $A = \\text{Spam}$  \n",
    "- $B = \\text{Words in the email}$\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) = \\frac{P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Words})}\n",
    "$$\n",
    "\n",
    "Then: \n",
    "\n",
    "- We know how often certain words appear in spam:  \n",
    "  → $P(\\text{Words} \\mid \\text{Spam})$\n",
    "\n",
    "- We know the overall proportion of spam:  \n",
    "  → $P(\\text{Spam})$\n",
    "\n",
    "Bayes’ Theorem helps us compute $P(\\text{Spam} \\mid \\text{Words})$ — the chance an email is spam **given** the words it contains.\n",
    "\n",
    "We usually compare two classes:\n",
    "\n",
    "$$\n",
    "\\frac{P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Words})}\n",
    "\\quad \\text{vs} \\quad\n",
    "\\frac{P(\\text{Words} \\mid \\text{Not Spam}) \\cdot P(\\text{Not Spam})}{P(\\text{Words})}\n",
    "$$\n",
    "\n",
    "Since the denominator is the same, it cancels out:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})\n",
    "$$\n",
    "\n",
    "> The symbol $\\propto$ means \"proportional to\" — we're only interested in which class has the higher score, so we can safely ignore the shared denominator.\n",
    "\n",
    "Next, let’s simplify how we calculate the probability of the words.\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "Just like assuming each coin flip is independent makes probability easier to compute, we assume all words are **conditionally independent** given the class — so the overall probability can be calculated by simply multiplying each word’s probability:\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam}) = P(x_1 \\mid \\text{Spam}) \\cdot P(x_2 \\mid \\text{Spam}) \\cdot \\cdots \\cdot P(x_n \\mid \\text{Spam}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{Spam})\n",
    "$$\n",
    "\n",
    "> The symbol $\\prod$ is the product symbol — like $\\sum$ but for multiplication.\n",
    "\n",
    "It's called *Naive* because this independence assumption is rarely true in real language — but it works surprisingly well in practice.\n",
    "\n",
    "Now let’s look at how to actually estimate these word probabilities from data.\n",
    "\n",
    "### Calculating Word Probabilities in Spam Emails\n",
    "\n",
    "Now that we assume each word is independent, we need a way to calculate how likely each word appears in spam emails.\n",
    "\n",
    "Here’s how it works:\n",
    "\n",
    "#### 1. Learn Word Probabilities from Training Data\n",
    "\n",
    "From the training data, we count how often each word appears in spam emails.\n",
    "\n",
    "Let’s define:\n",
    "\n",
    "- $x_i = 1$ means the $i$-th word is **present** in the email  \n",
    "- $x_i = 0$ means the $i$-th word is **absent**\n",
    "\n",
    "We want to estimate:\n",
    "\n",
    "$$\n",
    "\\theta_i = P(x_i = 1 \\mid \\text{Spam}) = \\frac{\\text{Number of spam emails containing word } i}{\\text{Total number of spam emails}}\n",
    "$$\n",
    "\n",
    "This gives us the probability that a spam email **contains** word $i$ at least once.\n",
    "\n",
    "##### To Avoid Zero Probability: Laplace Smoothing\n",
    "\n",
    "Sometimes a word might never appear in spam emails, which would give us a probability of 0 and cause the whole product to collapse.\n",
    "\n",
    "To fix that, we use **Laplace smoothing**:\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{\\text{Number of spam emails containing word } i + \\alpha}{\\text{Total number of spam emails} + 2\\alpha}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is a small constant (usually 1)\n",
    "- $2\\alpha$ accounts for both word **present** and **absent** cases\n",
    "\n",
    "#### 2. Check the Email for Each Word\n",
    "\n",
    "For a new email, we check each word $x_i$:\n",
    "\n",
    "- If the word **is present**: we use $\\theta_i$  \n",
    "- If the word **is absent**: we use $1 - \\theta_i$\n",
    "\n",
    "We can express both in a single formula:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{Spam}) = \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Why it works:\n",
    "\n",
    "- If $x_i = 1$:  \n",
    "  $P(x_i \\mid \\text{Spam}) = \\theta_i$\n",
    "\n",
    "- If $x_i = 0$:  \n",
    "  $P(x_i \\mid \\text{Spam}) = 1 - \\theta_i$\n",
    "\n",
    "#### 3. Multiply All Word Probabilities Together\n",
    "\n",
    "Since we assumed all words are conditionally independent:\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam}) = \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "So we multiply each word’s probability — whether it’s present or not — to get the full likelihood for the email.\n",
    "\n",
    "This is how Bernoulli Naive Bayes models text classification.\n",
    "\n",
    "### Combine with Prior Probability\n",
    "\n",
    "We also multiply by the prior probability of spam:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam}) = \\frac{\\text{Number of spam emails}}{\\text{Total number of emails}}\n",
    "$$\n",
    "\n",
    "So the full expression becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Spam}) \\cdot \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "At this point, we have everything — but there’s still a problem when we try to compute this on a real machine.\n",
    "\n",
    "### But There’s a Problem: Underflow\n",
    "\n",
    "Multiplying many small probabilities can make the result **extremely tiny** — too small for the computer to represent accurately.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "0.9 \\cdot 0.6 \\cdot 0.1 \\cdot 0.02 = 0.00108\n",
    "$$\n",
    "\n",
    "With hundreds of terms, the value might become so small it gets rounded to **0**. This is called **underflow**, and it breaks our calculation.\n",
    "\n",
    "#### The Fix: Use Logarithms\n",
    "\n",
    "Instead of multiplying probabilities, we take the **log** and add them — this avoids underflow and is easier to compute.\n",
    "\n",
    "#### Log Rules We’ll Use\n",
    "\n",
    "We’ll use these two key log rules:\n",
    "\n",
    "- $\\log(a \\cdot b) = \\log a + \\log b$  \n",
    "- $\\log(a^b) = b \\cdot \\log a$  \n",
    "\n",
    "#### Why It Works: A Simple Example\n",
    "\n",
    "Let’s take the log of the earlier multiplication:\n",
    "\n",
    "$$\n",
    "\\log(0.9 \\cdot 0.6 \\cdot 0.1 \\cdot 0.02) = \\log(0.9) + \\log(0.6) + \\log(0.1) + \\log(0.02)\n",
    "$$\n",
    "\n",
    "Approximate values:\n",
    "\n",
    "$$\n",
    "= -0.105 + (-0.222) + (-1.0) + (-1.699) = -3.026\n",
    "$$\n",
    "\n",
    "Instead of a tiny number like **0.00108**, we now get a manageable value: **–3.026**\n",
    "\n",
    "#### Apply to Bernoulli Naive Bayes\n",
    "\n",
    "We start from:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Spam}) \\cdot \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Take the log:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\log \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Convert the product into a sum:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\log\\left( \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i} \\right)\n",
    "$$\n",
    "\n",
    "Break inside the log:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\left[ \\log(\\theta_i^{x_i}) + \\log((1 - \\theta_i)^{1 - x_i}) \\right]\n",
    "$$\n",
    "\n",
    "Apply the power rule:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\left[ x_i \\log \\theta_i + (1 - x_i) \\log(1 - \\theta_i) \\right]\n",
    "$$\n",
    "\n",
    "## Let's Code It\n",
    "\n",
    "Now that we’ve covered all the logic behind Bernoulli Naive Bayes — from probability theory to handling underflow with logs — it’s time to **implement it from scratch**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc3f55-c2d3-4ba8-b02f-db4f94db45a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch accuracy = 0.8818\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 3. From‑scratch Bernoulli Naive Bayes (now with “what is happening” comments)\n",
    "# --------------------------------------------------\n",
    "class MyBernoulliNB:\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        # α = 1  →  classic Laplace smoothing (“add‑one”)\n",
    "        # avoids zero probabilities when a word never appears in one class\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # ==================== TRAIN ====================\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        X  ─ shape (n_emails, n_words)\n",
    "             1 = word appears at least once, 0 = absent\n",
    "        y  ─ shape (n_emails,)\n",
    "             0 = ham, 1 = spam\n",
    "        \"\"\"\n",
    "        n_emails, n_words = X.shape\n",
    "        self.classes_ = np.array([0, 1])        # fixed order: [ham, spam]\n",
    "\n",
    "        # ---------- 1. PRIOR P(class) ----------\n",
    "        # How many emails are ham / spam?\n",
    "        class_counts        = np.bincount(y, minlength=2)      # shape (2,)\n",
    "        self.log_prior_     = np.log(class_counts / n_emails)  # log for later addition\n",
    "\n",
    "        # ---------- 2. WORD PROBABILITIES θ ----------\n",
    "        # word_present[c, j] = how many emails of class c contain word j\n",
    "        word_present = np.array([\n",
    "            X[y == 0].sum(axis=0),   # ham row  (shape (n_words,))\n",
    "            X[y == 1].sum(axis=0)    # spam row\n",
    "        ])                            # overall shape (2, n_words)\n",
    "\n",
    "        # emails_per_class[0] = #ham emails, [1] = #spam emails\n",
    "        emails_per_class = class_counts.reshape(-1, 1)          # shape (2, 1)\n",
    "\n",
    "        # θ = smoothed fraction of emails *with* the word (per class & word)\n",
    "        theta = (word_present + self.alpha) / (emails_per_class + 2 * self.alpha)\n",
    "        #         ↑ numerator            ↑ denominator (present + absent)\n",
    "\n",
    "        # Store both log θ   and   log (1‑θ)\n",
    "        self.log_theta_present_ = np.log(theta)      # when word flag = 1\n",
    "        self.log_theta_absent_  = np.log(1 - theta)  # when word flag = 0\n",
    "        return self\n",
    "\n",
    "    # =================== PREDICT ===================\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Score matrix logic (vectorised):\n",
    "\n",
    "        scores[:, 0]  = ham_log_prior\n",
    "                      + sum_i [word_i_present * log θ_ham,i\n",
    "                               + word_i_absent  * log (1‑θ_ham,i)]\n",
    "\n",
    "        same for column 1 (spam).  The bigger column wins.\n",
    "        \"\"\"\n",
    "\n",
    "        #  X              shape: (n_test, n_words)\n",
    "        #  log_theta_.T   shape: (n_words, 2)\n",
    "        #  X @ ...        →     (n_test, 2)    (sum of present‑word logs)\n",
    "        scores = (\n",
    "            self.log_prior_                               # broadcast to (n_test, 2)\n",
    "            + X       @ self.log_theta_present_.T         # add logs for present words\n",
    "            + (1 - X) @ self.log_theta_absent_.T          # add logs for absent words\n",
    "        )\n",
    "        # For each email (row) pick class index with higher score\n",
    "        return self.classes_[scores.argmax(axis=1)]\n",
    "\n",
    "my_nb      = MyBernoulliNB(alpha=1.0).fit(X_train, y_train)\n",
    "y_pred_my  = my_nb.predict(X_test)\n",
    "acc_my     = accuracy_score(y_test, y_pred_my)\n",
    "print(f\"scratch accuracy = {acc_my:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff17e8-febc-4f88-87c2-675edc20ac91",
   "metadata": {},
   "source": [
    "The scratch model hits an accuracy of **0.8818**, matching scikit‑learn.  \n",
    "\n",
    "Next, let’s plot a **confusion matrix** to see which e‑mails were classified correctly and where mistakes happen.\n",
    "\n",
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abcd7c0-1c6b-49f1-b0d2-d3c89d2838aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 4. Visual diagnostics (no 2‑D decision boundary)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- Confusion matrices ------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for ax, preds, title in zip(\n",
    "    axes, [y_pred_sk, y_pred_my], [\"scikit‑learn\", \"scratch\"]\n",
    "):\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, preds, cmap=\"Greens\", ax=ax, colorbar=False\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "fig.suptitle(\"Bernoulli NB – Confusion Matrices\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6531b19-0251-4d5e-b511-6a821b258831",
   "metadata": {},
   "source": [
    "## It Works!!\n",
    "\n",
    "The predictions from our scratch **Bernoulli Naive Bayes** implementation line up perfectly with the results from **scikit‑learn**.\n",
    "\n",
    "This confirms that our logic — counting word occurrences, adding Laplace smoothing, summing log‑likelihoods, and picking the class with the higher total — behaves exactly as expected.\n",
    "\n",
    "We've successfully built **Bernoulli Naive Bayes** from the ground up!"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-18T05:26:02.906712+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-18T23:38:11.477690+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
