{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253f824e-fa49-420e-8124-d01cb796a8f7",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "## Probability Basics\n",
    "\n",
    "Probability tells us how likely something is to happen. The result is always between 0 and 1.\n",
    "\n",
    "In this section, we’ll learn 3 essential concepts:\n",
    "\n",
    "1. Marginal Probability  \n",
    "2. Joint Probability  \n",
    "3. The Product Rule (with Conditional Probability)\n",
    "\n",
    "We’ll use **coin flips** for all examples.\n",
    "\n",
    "### 1. Marginal Probability\n",
    "\n",
    "This is the probability of a single event happening.\n",
    "\n",
    "**Example — Flipping a coin:**\n",
    "\n",
    "- The probability of getting Heads:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "- The probability of getting Tails:\n",
    "\n",
    "$$\n",
    "P(\\text{Tails}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "These are called **marginal probabilities** — they only involve one event.\n",
    "\n",
    "### 2. Joint Probability\n",
    "\n",
    "This is the probability of **two events happening together**:\n",
    "\n",
    "$$\n",
    "P(A \\cap B)\n",
    "$$\n",
    "\n",
    "**Example — Flipping two coins:**\n",
    "\n",
    "- What’s the probability of getting Heads on the first **and** Heads on the second?\n",
    "\n",
    "If the flips are **independent**:\n",
    "\n",
    "$$\n",
    "P(\\text{Heads on both}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "So for **independent events only**, the joint probability is:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "If the events are **not independent**, we need the **product rule**.\n",
    "\n",
    "### 3. The Product Rule (with Conditional Probability)\n",
    "\n",
    "The **product rule** handles both independent and dependent events:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A \\mid B) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "This introduces **conditional probability**:\n",
    "\n",
    "- $P(A \\mid B)$ means \"the probability of $A$, assuming $B$ has already happened.\"\n",
    "\n",
    "**Example — Dependent coin flips:**\n",
    "\n",
    "- If the first flip is Heads, the second flip is Heads **75%** of the time.\n",
    "- $P(\\text{First = Heads}) = \\frac{1}{2}$\n",
    "- $P(\\text{Second = Heads} \\mid \\text{First = Heads}) = 0.75$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(\\text{Both Heads}) = 0.75 \\cdot \\frac{1}{2} = 0.375\n",
    "$$\n",
    "\n",
    "### From Product Rule to Bayes’ Theorem\n",
    "\n",
    "We can also write the product rule the other way:\n",
    "\n",
    "$$\n",
    "P(B \\cap A) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Since $P(A \\cap B) = P(B \\cap A)$, we can set them equal:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Now divide both sides by $P(B)$:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This is **Bayes’ Theorem** — a way to reverse the direction of conditional probability.\n",
    "\n",
    "## Bayes' Theorem for Classification\n",
    "\n",
    "Bayes’ Theorem helps us answer questions like:\n",
    "\n",
    "> \"Given some evidence, how likely is a hypothesis?\"\n",
    "\n",
    "The formula:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Now let:\n",
    "- $A = \\text{Spam}$  \n",
    "- $B = \\text{Words in the email}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) = \\frac{P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Words})}\n",
    "$$\n",
    "\n",
    "### Why This Is Useful\n",
    "\n",
    "In email spam detection:\n",
    "\n",
    "- We know how often certain words appear in spam:  \n",
    "  → $P(\\text{Words} \\mid \\text{Spam})$\n",
    "\n",
    "- We know the overall proportion of spam:  \n",
    "  → $P(\\text{Spam})$\n",
    "\n",
    "But we want:\n",
    "\n",
    "- The probability an email is spam **given** its words:  \n",
    "  → $P(\\text{Spam} \\mid \\text{Words})$\n",
    "\n",
    "Bayes’ Theorem reverses the direction — from \"given spam, what's likely to appear\" to \"given what appears, how likely is spam?\"\n",
    "\n",
    "### Why It's Called \"Naive\" Bayes\n",
    "\n",
    "If an email has 10 words, computing:\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam})\n",
    "$$\n",
    "\n",
    "...requires modeling word combinations, which is hard.\n",
    "\n",
    "So we make a **naive assumption**:\n",
    "\n",
    "> All words are conditionally independent given the class.\n",
    "\n",
    "Even though the assumption is unrealistic, it works well in practice — hence the name **Naive Bayes**.\n",
    "\n",
    "### Why We Can Ignore $P(\\text{Words})$\n",
    "\n",
    "We usually compare two classes:\n",
    "\n",
    "$$\n",
    "\\frac{P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Words})}\n",
    "\\quad \\text{vs} \\quad\n",
    "\\frac{P(\\text{Words} \\mid \\text{Not Spam}) \\cdot P(\\text{Not Spam})}{P(\\text{Words})}\n",
    "$$\n",
    "\n",
    "Since the denominator is the same, it cancels out:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Words} \\mid \\text{Spam}) \\cdot P(\\text{Spam})\n",
    "$$\n",
    "\n",
    "## The Naive Assumption\n",
    "\n",
    "We assume all words are **conditionally independent** given the class:\n",
    "\n",
    "$$\n",
    "P(\\text{Words} \\mid \\text{Spam}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{Spam})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i = 1$ if word $i$ is present\n",
    "- $x_i = 0$ otherwise\n",
    "\n",
    "### What is $\\prod$?\n",
    "\n",
    "$\\prod$ is the product symbol — like $\\sum$ but for multiplication:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^{n} a_i = a_1 \\cdot a_2 \\cdot \\cdots \\cdot a_n\n",
    "$$\n",
    "\n",
    "## Estimating $P(x_i \\mid \\text{Spam})$\n",
    "\n",
    "We define:\n",
    "\n",
    "$$\n",
    "\\theta_i = P(x_i = 1 \\mid \\text{Spam})\n",
    "$$\n",
    "\n",
    "Using Laplace smoothing:\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{\\text{number of spam emails containing word } i + \\alpha}{\\text{total number of spam emails} + 2\\alpha}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is a small constant (typically 1) to avoid zero probability\n",
    "- $2\\alpha$ accounts for both present and absent cases\n",
    "\n",
    "### Why Use $\\theta_i$ and $1 - \\theta_i$\n",
    "\n",
    "We want to handle two cases:\n",
    "\n",
    "- If $x_i = 1$, use $\\theta_i$\n",
    "- If $x_i = 0$, use $1 - \\theta_i$\n",
    "\n",
    "So we write both in one formula:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{Spam}) = \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "## Final Bernoulli Naive Bayes Formula\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Words}) \\propto P(\\text{Spam}) \\cdot \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- $x = [1, 0, 1]$  \n",
    "- $\\theta = [0.8, 0.4, 0.6]$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(\\text{Spam} \\mid x) \\propto P(\\text{Spam}) \\cdot 0.8 \\cdot (1 - 0.4) \\cdot 0.6 = P(\\text{Spam}) \\cdot 0.8 \\cdot 0.6 \\cdot 0.6\n",
    "$$\n",
    "\n",
    "## Use Log to Prevent Underflow\n",
    "\n",
    "Multiplying small numbers leads to underflow:\n",
    "\n",
    "$$\n",
    "0.9 \\cdot 0.6 \\cdot 0.1 \\cdot 0.02 = 0.00108\n",
    "$$\n",
    "\n",
    "Over many features, this approaches zero.\n",
    "\n",
    "### Logarithmic Form:\n",
    "\n",
    "Convert products to sums:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{Spam} \\mid \\text{Words}) \\propto \\log P(\\text{Spam}) + \\sum_{i=1}^{n} \\left[ x_i \\log \\theta_i + (1 - x_i) \\log (1 - \\theta_i) \\right]\n",
    "$$\n",
    "\n",
    "This is numerically stable and used in real implementations."
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-18T05:26:02.906712+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-18T05:26:02.906712+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
