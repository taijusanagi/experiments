{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325103d1-5e5f-46cc-96b5-eee45afef2f5",
   "metadata": {},
   "source": [
    "# Entropy and Information Gain\n",
    "\n",
    "## Overview\n",
    "\n",
    "When building a Decision Tree, we aim to split data into increasingly pure groups.  \n",
    "But how can we measure \"purity\" or \"impurity\" mathematically?\n",
    "\n",
    "One powerful way is using **entropy**, a concept from information theory.  \n",
    "Entropy measures how **mixed** or **uncertain** a group is.  \n",
    "When we split data, we use **information gain** — the reduction in entropy — to decide the best split.\n",
    "\n",
    "## 1. Understanding Purity and Impurity\n",
    "\n",
    "*Imagine a bucket of marbles where each **color = a class**.*\n",
    "\n",
    "- If all marbles are **blue**, the bucket is **perfectly pure** — you always guess “blue” and be right.\n",
    "- If the marbles are **half blue, half red**, the bucket is **impure** — your guess will be wrong half the time.\n",
    "\n",
    "The more mixed the bucket, the **higher the uncertainty**.  \n",
    "We want to find questions (splits) that move us toward **lower uncertainty** — smaller entropy.\n",
    "\n",
    "## 2. What is Entropy?\n",
    "\n",
    "**Plain language definition:**  \n",
    "> \"Entropy measures how much **surprise** or **uncertainty** there is when picking an item at random.\"\n",
    "\n",
    "- If the bucket is pure (all one class), no surprise → entropy = 0.\n",
    "- If the bucket is highly mixed, lots of surprise → higher entropy.\n",
    "\n",
    "### Entropy Formula\n",
    "\n",
    "Let $p_k$ be the proportion of samples in class $k$.  \n",
    "If there are $K$ classes, the entropy $H$ is:\n",
    "\n",
    "$$\n",
    "H = -\\sum_{k=1}^K p_k \\log_2 p_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\log_2$ is the logarithm base 2 (information measured in \"bits\"),\n",
    "- By convention, $0 \\log_2 0 = 0$.\n",
    "\n",
    "### Examples\n",
    "\n",
    "- **Pure bucket** (all one class, say $p_k=1$):\n",
    "\n",
    "$$\n",
    "H = -1 \\times \\log_2 1 = 0\n",
    "$$\n",
    "\n",
    "✅ No uncertainty.\n",
    "\n",
    "- **Two classes evenly mixed** ($p_1 = p_2 = 0.5$):\n",
    "\n",
    "$$\n",
    "H = -\\left(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5\\right) = 1.0\n",
    "$$\n",
    "\n",
    "✅ Maximum uncertainty for 2 classes.\n",
    "\n",
    "## 3. How Splitting Affects Entropy\n",
    "\n",
    "When a question splits a parent group into left and right children:\n",
    "\n",
    "- $N_P$: number of samples in parent\n",
    "- $N_L$, $N_R$: numbers of samples in left/right child\n",
    "- $H_P$: entropy of parent\n",
    "- $H_L$, $H_R$: entropies of left/right children\n",
    "\n",
    "The **weighted average entropy after the split** is:\n",
    "\n",
    "$$\n",
    "\\text{Weighted Entropy} = \\frac{N_L}{N_P} H_L + \\frac{N_R}{N_P} H_R\n",
    "$$\n",
    "\n",
    "## 4. What is Information Gain?\n",
    "\n",
    "**Information gain** measures **how much entropy decreases** after a split:\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = H_P - \\left( \\frac{N_L}{N_P} H_L + \\frac{N_R}{N_P} H_R \\right)\n",
    "$$\n",
    "\n",
    "✅ Interpretation:\n",
    "\n",
    "- **High information gain** → split made the groups much purer.\n",
    "- **Low information gain** → split did not improve purity much.\n",
    "\n",
    "When building a tree, we choose the split with the **highest information gain** at each step.\n",
    "\n",
    "## 5. Growing a Decision Tree with Entropy and Information Gain\n",
    "\n",
    "1. Start with all data in the **root node**.\n",
    "2. For every possible question (feature + threshold):\n",
    "   - Calculate how the split would divide the data.\n",
    "   - Calculate the information gain from the split.\n",
    "3. Choose the question with the **highest information gain**.\n",
    "4. Split the node into left/right children.\n",
    "5. Recursively repeat steps 2–4 for each child.\n",
    "6. **Stop** when:\n",
    "   - A node is pure (entropy = 0), or\n",
    "   - Other limits are reached (e.g., `max_depth`, `min_samples_leaf`).\n",
    "\n",
    "## 6. Making Predictions with the Tree\n",
    "\n",
    "To classify a new input:\n",
    "\n",
    "1. Start at the **root node**.\n",
    "2. Ask the stored yes/no question.\n",
    "3. Follow the **yes** or **no** branch.\n",
    "4. Repeat until reaching a **leaf node**.\n",
    "5. Output the most common class label in that leaf.\n",
    "\n",
    "✅ Like playing \"20 Questions\" — each answer brings you closer to the final class.\n",
    "\n",
    "# Final Thoughts\n",
    "\n",
    "- **Entropy** measures how mixed a group is.\n",
    "- **Information gain** tells us how much a split reduces that mixing.\n",
    "- Decision Trees use information gain to decide which questions to ask.\n",
    "\n",
    "Building trees with **entropy and information gain** helps models split the data intelligently, leading to better classification accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-28T05:47:10.777916+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-28T05:47:10.777916+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
