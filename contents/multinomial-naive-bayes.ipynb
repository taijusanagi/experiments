{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7f435a-fddc-435a-995d-d55fd2bb4dff",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "This note introduces the **Multinomial Naive Bayes** algorithm using `scikit‑learn`, explains the step‑by‑step logic behind how it works, and then demonstrates a from‑scratch implementation to show that the core idea is simple and easy to build.\n",
    "\n",
    "## What is Multinomial Naive Bayes?\n",
    "\n",
    "Multinomial Naive Bayes is like keeping a **bag‑of‑words count** for every e‑mail, for example:\n",
    "\n",
    "- How many times does the word *money* appear in the message?\n",
    "\n",
    "This single‑word example is just one feature — in practice the model counts how often **each word in the vocabulary** appears in the email. Each word’s count adds a score toward **spam** or **ham**.\n",
    "\n",
    "After summing all those scores, the class with the higher total wins.\n",
    "\n",
    "It learns these scores from past data — how often each word appears in spam and ham messages. Using counts (not just presence/absence) makes this model better for longer documents where the same word might appear multiple times.\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "- **Use `scikit‑learn`** to demonstrate how Multinomial Naive Bayes works in practice  \n",
    "- **Explain the logic behind it** in an intuitive way (scorecard view, Laplace smoothing, using logs instead of tiny products)  \n",
    "- **Show how to implement the same idea step by step from scratch**  \n",
    "\n",
    "Let’s dive into the details to understand how it works and how to implement it ourselves.\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2d88a-df58-4566-a123-a8fa6b9ae462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18846 documents across 20 categories\n",
      "X shape: (18846, 10000)\n",
      "Sample vocabulary words: ['sure', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'massacre']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X_text = newsgroups.data\n",
    "y = newsgroups.target\n",
    "target_names = newsgroups.target_names\n",
    "\n",
    "print(f\"Loaded {len(X_text)} documents across {len(target_names)} categories\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)  # limit vocab size\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # sparse matrix shape (n_docs, n_words)\n",
    "print(\"Sample vocabulary words:\", list(vectorizer.vocabulary_.keys())[:10])  # show a few words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568356f1-3564-4533-a305-a91c5a5f1677",
   "metadata": {},
   "source": [
    "## Data Observation\n",
    "\n",
    "`X` is a sparse matrix with shape `(18846, 10000)`, where each row represents a document and each column represents a word from the 10,000 most frequent vocabulary terms (after removing stopwords).\n",
    "\n",
    "Each value in `X` is the **raw count** of how many times that word appears in the corresponding document.\n",
    "\n",
    "For example:\n",
    "- `X[0, 5123] = 2` means the 5123rd word in the vocabulary appears twice in document 0.\n",
    "- Most values are zero due to the sparsity of natural language.\n",
    "\n",
    "The target `y` contains the category labels (0–19), each corresponding to a newsgroup topic such as `'rec.autos'`, `'sci.space'`, or `'talk.politics.misc'`.\n",
    "\n",
    "By learning how often each word appears across different categories, **Multinomial Naive Bayes** estimates class‑conditional probabilities and predicts the most likely category for each new document.\n",
    "\n",
    "## Implement with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89c91c-50e6-4e7e-9411-86cf4d8abb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn accuracy = 0.6719\n"
     ]
    }
   ],
   "source": [
    "sk_model = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk  = sk_model.predict(X_test)\n",
    "y_proba_sk = sk_model.predict_proba(X_test)  # shape: (n_samples, n_classes)\n",
    "\n",
    "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
    "print(f\"scikit-learn accuracy = {acc_sk:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b0dcc-1735-4eae-85dd-ac6204d3ce01",
   "metadata": {},
   "source": [
    "## Behind the Scenes: Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes is a way to classify documents by looking at the **words inside them** and **how many times** each word appears.\n",
    "\n",
    "Unlike Bernoulli Naive Bayes (which only checks if a word is present), Multinomial Naive Bayes uses **raw word counts**.\n",
    "\n",
    "### Bayes' Theorem for Classification\n",
    "\n",
    "We want to know:\n",
    "\n",
    "> “What is the probability this document belongs to a class, given the words in it?”\n",
    "\n",
    "We write this as:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid \\text{words}) \\propto P(\\text{words} \\mid \\text{class}) \\cdot P(\\text{class})\n",
    "$$\n",
    "\n",
    "We calculate this score for every possible class and pick the one with the highest result.\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "We assume all words in a document are conditionally independent, given the class.\n",
    "\n",
    "That means:\n",
    "\n",
    "$$\n",
    "P(\\text{words} \\mid \\text{class}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{class})^{x_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is how many times word $i$ appears in the document\n",
    "- $P(x_i \\mid \\text{class})$ is the probability of that word in that class\n",
    "- $\\prod$ means multiply all these probabilities\n",
    "\n",
    "### Input Format\n",
    "\n",
    "Each input document is turned into a list of word counts:  \n",
    "- \"data science is fun\" → `[1, 1, 1, 0, 0, 0, …]`  \n",
    "- \"science science science\" → `[0, 3, 0, 0, 0, …]`\n",
    "\n",
    "This is why we write:\n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{N}\n",
    "$$\n",
    "\n",
    "Which means each feature $x_i$ is a **whole number** (0 or more).\n",
    "\n",
    "### Estimating Word Probabilities from Training Data\n",
    "\n",
    "To calculate $P(x_i \\mid \\text{class})$, we look at **all documents** in a class and count how often each word appears.\n",
    "\n",
    "Let:\n",
    "- $N_{i,c}$ = total number of times word $i$ appears in class $c$\n",
    "- $N_c$ = total number of all word occurrences in class $c$\n",
    "- $V$ = vocabulary size (number of different words)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{class}) = \\frac{N_{i,c} + \\alpha}{N_c + \\alpha V}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is a smoothing value (usually 1) to prevent zero probabilities.\n",
    "\n",
    "> This tells us: “If we randomly pick a word from all the training documents in this class, what's the chance that it's word $i$?”\n",
    "\n",
    "In other words, it’s the **relative frequency** of the word in that class.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Imagine we are training a classifier for two classes: `sports` and `tech`.\n",
    "\n",
    "We focus on just **3 words** in the vocabulary: `game`, `code`, `team`.\n",
    "\n",
    "In all training documents for `sports`:\n",
    "\n",
    "- `game` appears **10 times**  \n",
    "- `code` appears **0 times**  \n",
    "- `team` appears **5 times**\n",
    "\n",
    "So:\n",
    "- $N_{\\text{game, sports}} = 10$\n",
    "- $N_{\\text{code, sports}} = 0$\n",
    "- $N_{\\text{team, sports}} = 5$\n",
    "- Total $N_{\\text{sports}} = 10 + 0 + 5 = 15$\n",
    "\n",
    "Using $\\alpha = 1$ and $V = 3$:\n",
    "\n",
    "$$\n",
    "P(\\text{game} \\mid \\text{sports}) = \\frac{10 + 1}{15 + 3} = \\frac{11}{18} \\approx 0.611 \\\\\n",
    "P(\\text{code} \\mid \\text{sports}) = \\frac{0 + 1}{15 + 3} = \\frac{1}{18} \\approx 0.056 \\\\\n",
    "P(\\text{team} \\mid \\text{sports}) = \\frac{5 + 1}{15 + 3} = \\frac{6}{18} = 0.333\n",
    "$$\n",
    "\n",
    "This tells the model:\n",
    "- \"game\" is very common in sports documents\n",
    "- \"code\" is almost never seen in sports\n",
    "- \"team\" is fairly common\n",
    "\n",
    "### Combine with Prior Probability\n",
    "\n",
    "We also multiply by the prior probability of each class:\n",
    "\n",
    "$$\n",
    "P(\\text{class}) = \\frac{\\text{Number of training documents in this class}}{\\text{Total number of training documents}}\n",
    "$$\n",
    "\n",
    "So the full expression for the class score becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid \\text{Words}) \\propto P(\\text{class}) \\cdot \\prod_{i=1}^{n} P(x_i \\mid \\text{class})^{x_i}\n",
    "$$\n",
    "\n",
    "> 🔄 This is where **Multinomial Naive Bayes** differs from **Bernoulli Naive Bayes**:\n",
    "\n",
    "- **BernoulliNB** multiplies:  \n",
    "  $$\n",
    "  \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "  $$  \n",
    "  (for binary word presence: 1 or 0)\n",
    "\n",
    "- **MultinomialNB** multiplies:  \n",
    "  $$\n",
    "  \\prod_{i=1}^{n} P(x_i \\mid \\text{class})^{x_i}\n",
    "  $$  \n",
    "  (where $x_i$ is a count — 0, 1, 2, 3, ...)\n",
    "\n",
    "So instead of using $(1 - \\theta_i)$ for absent words like in BernoulliNB,  \n",
    "MultinomialNB just **ignores words that don’t appear ($x_i = 0$)** and **focuses more on words that repeat**.\n",
    "\n",
    "> 🧠 Intuition:\n",
    "\n",
    "- For BernoulliNB, every word either contributes a \"yes\" or \"no\" (present or absent)\n",
    "- For MultinomialNB, only present words contribute — and **more frequent words contribute more**\n",
    "\n",
    "We now have a complete score expression in probability form.  \n",
    "Before calculating it on a computer, we’ll apply logarithms to avoid underflow.\n",
    "\n",
    "### Final Scoring Formula (with Logs)\n",
    "\n",
    "When classifying a new document, we compute:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{class} \\mid X) \\propto \\log P(\\text{class}) + \\sum_{i=1}^{n} x_i \\cdot \\log P(x_i \\mid \\text{class})\n",
    "$$\n",
    "\n",
    "Each word’s count $x_i$ acts like a **weight**, multiplying the impact of that word’s log-probability.\n",
    "\n",
    "The model picks the class with the highest final score.\n",
    "\n",
    "## Let's Code It\n",
    "Now that we understand how it works, let’s implement it from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc608d64-48fa-4926-a36d-56e631fa12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch accuracy = 0.6719\n"
     ]
    }
   ],
   "source": [
    "class MyMultinomialNB:\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        # α = 1  → classic Laplace smoothing (“add-one”)\n",
    "        # prevents zero probabilities for unseen words\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # ==================== TRAIN ====================\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        X  ─ shape (n_docs, n_words)\n",
    "             Each value = how many times word i appears in the document\n",
    "        y  ─ shape (n_docs,)\n",
    "             Each value = class label (e.g. 0 = ham, 1 = spam)\n",
    "        \"\"\"\n",
    "        n_docs, n_words = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # ---------- 1. PRIOR P(class) ----------\n",
    "        class_counts = np.bincount(y, minlength=n_classes)\n",
    "        self.log_prior_ = np.log(class_counts / n_docs)  # shape (n_classes,)\n",
    "\n",
    "        # ---------- 2. WORD COUNTS ----------\n",
    "        # word_counts[c, j] = total number of times word j appears in class c\n",
    "        word_counts = np.zeros((n_classes, n_words))\n",
    "\n",
    "        for c in self.classes_:\n",
    "            X_class = X[y == c]\n",
    "            word_counts[c] = X_class.sum(axis=0)  # sum over documents in class\n",
    "\n",
    "        # total_words[c] = total number of all word tokens in class c\n",
    "        total_words = word_counts.sum(axis=1, keepdims=True)  # shape (n_classes, 1)\n",
    "\n",
    "        # ---------- 3. SMOOTHED WORD PROBABILITIES ----------\n",
    "        # θ = probability of word j given class c\n",
    "        # shape: (n_classes, n_words)\n",
    "        self.log_theta_ = np.log(\n",
    "            (word_counts + self.alpha) /\n",
    "            (total_words + self.alpha * n_words)\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    # =================== PREDICT ===================\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        For each document:\n",
    "        - Compute log‑score for each class:\n",
    "            log P(class) + sum_i x_i * log P(word_i | class)\n",
    "        - Choose class with highest score\n",
    "        \"\"\"\n",
    "        # X: shape (n_test_docs, n_words)\n",
    "        # log_theta_: shape (n_classes, n_words)\n",
    "\n",
    "        # Compute score matrix: (n_test_docs, n_classes)\n",
    "        scores = X @ self.log_theta_.T + self.log_prior_\n",
    "\n",
    "        return self.classes_[scores.argmax(axis=1)]\n",
    "\n",
    "my_nb = MyMultinomialNB(alpha=1.0).fit(X_train, y_train)\n",
    "y_pred_my = my_nb.predict(X_test)\n",
    "acc_my = accuracy_score(y_test, y_pred_my)\n",
    "print(f\"scratch accuracy = {acc_my:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543742e6-e2ba-44a4-8edb-fbcd0e943fb7",
   "metadata": {},
   "source": [
    "## It Works!\n",
    "\n",
    "The scratch model hits an accuracy of **0.6719**, matching `scikit-learn`.\n",
    "\n",
    "This confirms that the logic behind **Multinomial Naive Bayes** — counting word frequencies, applying Laplace smoothing, computing log-scores, and picking the class with the highest total — behaves exactly as expected.\n",
    "\n",
    "We’ve successfully built **Multinomial Naive Bayes** from the ground up!"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-21T04:23:37.599707+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-21T04:23:37.599707+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
