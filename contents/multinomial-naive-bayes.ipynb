{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7f435a-fddc-435a-995d-d55fd2bb4dff",
   "metadata": {},
   "source": [
    "# Multinomial¬†Naive¬†Bayes\n",
    "\n",
    "This note introduces the **Multinomial¬†Naive¬†Bayes** algorithm using `scikit‚Äëlearn`, explains the step‚Äëby‚Äëstep logic behind how it works, and then demonstrates a from‚Äëscratch implementation to show that the core idea is simple and easy to build.\n",
    "\n",
    "## What¬†is¬†Multinomial¬†Naive¬†Bayes?\n",
    "\n",
    "Multinomial¬†Naive¬†Bayes is like keeping a **bag‚Äëof‚Äëwords count** for every e‚Äëmail, for example:\n",
    "\n",
    "- How many times does the word *money* appear in the message?\n",
    "\n",
    "This single‚Äëword example is just one feature ‚Äî in practice the model counts how often **each word in the vocabulary** appears in the email. Each word‚Äôs count adds a score toward **spam** or **ham**.\n",
    "\n",
    "After summing all those scores, the class with the higher total wins.\n",
    "\n",
    "It learns these scores from past data ‚Äî how often each word appears in spam and ham messages. Using counts (not just presence/absence) makes this model better for longer documents where the same word might appear multiple times.\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "- **Use `scikit‚Äëlearn`** to demonstrate how Multinomial¬†Naive¬†Bayes works in practice  \n",
    "- **Explain the logic behind it** in an intuitive way (scorecard view, Laplace smoothing, using logs instead of tiny products)  \n",
    "- **Show how to implement the same idea step by step from scratch**  \n",
    "\n",
    "Let‚Äôs dive into the details to understand how it works and how to implement it ourselves.\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2d88a-df58-4566-a123-a8fa6b9ae462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18846 documents across 20 categories\n",
      "X shape: (18846, 10000)\n",
      "Sample vocabulary words: ['sure', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'massacre']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X_text = newsgroups.data\n",
    "y = newsgroups.target\n",
    "target_names = newsgroups.target_names\n",
    "\n",
    "print(f\"Loaded {len(X_text)} documents across {len(target_names)} categories\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)  # limit vocab size\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # sparse matrix shape (n_docs, n_words)\n",
    "print(\"Sample vocabulary words:\", list(vectorizer.vocabulary_.keys())[:10])  # show a few words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568356f1-3564-4533-a305-a91c5a5f1677",
   "metadata": {},
   "source": [
    "## Data Observation\n",
    "\n",
    "`X` is a sparse matrix with shape `(18846, 10000)`, where each row represents a document and each column represents a word from the 10,000 most frequent vocabulary terms (after removing stopwords).\n",
    "\n",
    "Each value in `X` is the **raw count** of how many times that word appears in the corresponding document.\n",
    "\n",
    "For example:\n",
    "- `X[0, 5123] = 2` means the 5123rd word in the vocabulary appears twice in document 0.\n",
    "- Most values are zero due to the sparsity of natural language.\n",
    "\n",
    "The target `y` contains the category labels (0‚Äì19), each corresponding to a newsgroup topic such as `'rec.autos'`, `'sci.space'`, or `'talk.politics.misc'`.\n",
    "\n",
    "By learning how often each word appears across different categories, **Multinomial Naive Bayes** estimates class‚Äëconditional probabilities and predicts the most likely category for each new document.\n",
    "\n",
    "## Implement with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89c91c-50e6-4e7e-9411-86cf4d8abb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn accuracy = 0.6719\n"
     ]
    }
   ],
   "source": [
    "sk_model = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk  = sk_model.predict(X_test)\n",
    "y_proba_sk = sk_model.predict_proba(X_test)  # shape: (n_samples, n_classes)\n",
    "\n",
    "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
    "print(f\"scikit-learn accuracy = {acc_sk:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b0dcc-1735-4eae-85dd-ac6204d3ce01",
   "metadata": {},
   "source": [
    "## Behind the Scenes: Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes is a way to classify documents by looking at the **words inside them** and **how many times** each word appears.\n",
    "\n",
    "Unlike Bernoulli Naive Bayes (which only checks if a word is present), Multinomial Naive Bayes uses **raw word counts**.\n",
    "\n",
    "### Bayes' Theorem for Classification\n",
    "\n",
    "We want to know:\n",
    "\n",
    "> ‚ÄúWhat is the probability this document belongs to a class, given the words in it?‚Äù\n",
    "\n",
    "We write this as:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid \\text{words}) \\propto P(\\text{words} \\mid \\text{class}) \\cdot P(\\text{class})\n",
    "$$\n",
    "\n",
    "We calculate this score for every possible class and pick the one with the highest result.\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "We assume all words in a document are conditionally independent, given the class.\n",
    "\n",
    "That means:\n",
    "\n",
    "$$\n",
    "P(\\text{words} \\mid \\text{class}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{class})^{x_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is how many times word $i$ appears in the document\n",
    "- $P(x_i \\mid \\text{class})$ is the probability of that word in that class\n",
    "- $\\prod$ means multiply all these probabilities\n",
    "\n",
    "### Input Format\n",
    "\n",
    "Each input document is turned into a list of word counts:  \n",
    "- \"data science is fun\" ‚Üí `[1, 1, 1, 0, 0, 0, ‚Ä¶]`  \n",
    "- \"science science science\" ‚Üí `[0, 3, 0, 0, 0, ‚Ä¶]`\n",
    "\n",
    "This is why we write:\n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{N}\n",
    "$$\n",
    "\n",
    "Which means each feature $x_i$ is a **whole number** (0 or more).\n",
    "\n",
    "### Estimating Word Probabilities from Training Data\n",
    "\n",
    "To calculate $P(x_i \\mid \\text{class})$, we look at **all documents** in a class and count how often each word appears.\n",
    "\n",
    "Let:\n",
    "- $N_{i,c}$ = total number of times word $i$ appears in class $c$\n",
    "- $N_c$ = total number of all word occurrences in class $c$\n",
    "- $V$ = vocabulary size (number of different words)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid \\text{class}) = \\frac{N_{i,c} + \\alpha}{N_c + \\alpha V}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is a smoothing value (usually 1) to prevent zero probabilities.\n",
    "\n",
    "> This tells us: ‚ÄúIf we randomly pick a word from all the training documents in this class, what's the chance that it's word $i$?‚Äù\n",
    "\n",
    "In other words, it‚Äôs the **relative frequency** of the word in that class.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Imagine we are training a classifier for two classes: `sports` and `tech`.\n",
    "\n",
    "We focus on just **3 words** in the vocabulary: `game`, `code`, `team`.\n",
    "\n",
    "In all training documents for `sports`:\n",
    "\n",
    "- `game` appears **10 times**  \n",
    "- `code` appears **0 times**  \n",
    "- `team` appears **5 times**\n",
    "\n",
    "So:\n",
    "- $N_{\\text{game, sports}} = 10$\n",
    "- $N_{\\text{code, sports}} = 0$\n",
    "- $N_{\\text{team, sports}} = 5$\n",
    "- Total $N_{\\text{sports}} = 10 + 0 + 5 = 15$\n",
    "\n",
    "Using $\\alpha = 1$ and $V = 3$:\n",
    "\n",
    "$$\n",
    "P(\\text{game} \\mid \\text{sports}) = \\frac{10 + 1}{15 + 3} = \\frac{11}{18} \\approx 0.611 \\\\\n",
    "P(\\text{code} \\mid \\text{sports}) = \\frac{0 + 1}{15 + 3} = \\frac{1}{18} \\approx 0.056 \\\\\n",
    "P(\\text{team} \\mid \\text{sports}) = \\frac{5 + 1}{15 + 3} = \\frac{6}{18} = 0.333\n",
    "$$\n",
    "\n",
    "This tells the model:\n",
    "- \"game\" is very common in sports documents\n",
    "- \"code\" is almost never seen in sports\n",
    "- \"team\" is fairly common\n",
    "\n",
    "### Combine with Prior Probability\n",
    "\n",
    "We also multiply by the prior probability of each class:\n",
    "\n",
    "$$\n",
    "P(\\text{class}) = \\frac{\\text{Number of training documents in this class}}{\\text{Total number of training documents}}\n",
    "$$\n",
    "\n",
    "So the full expression for the class score becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid \\text{Words}) \\propto P(\\text{class}) \\cdot \\prod_{i=1}^{n} P(x_i \\mid \\text{class})^{x_i}\n",
    "$$\n",
    "\n",
    "> üîÑ This is where **Multinomial Naive Bayes** differs from **Bernoulli Naive Bayes**:\n",
    "\n",
    "- **BernoulliNB** multiplies:  \n",
    "  $$\n",
    "  \\prod_{i=1}^{n} \\theta_i^{x_i} \\cdot (1 - \\theta_i)^{1 - x_i}\n",
    "  $$  \n",
    "  (for binary word presence: 1 or 0)\n",
    "\n",
    "- **MultinomialNB** multiplies:  \n",
    "  $$\n",
    "  \\prod_{i=1}^{n} P(x_i \\mid \\text{class})^{x_i}\n",
    "  $$  \n",
    "  (where $x_i$ is a count ‚Äî 0, 1, 2, 3, ...)\n",
    "\n",
    "So instead of using $(1 - \\theta_i)$ for absent words like in BernoulliNB,  \n",
    "MultinomialNB just **ignores words that don‚Äôt appear ($x_i = 0$)** and **focuses more on words that repeat**.\n",
    "\n",
    "> üß† Intuition:\n",
    "\n",
    "- For BernoulliNB, every word either contributes a \"yes\" or \"no\" (present or absent)\n",
    "- For MultinomialNB, only present words contribute ‚Äî and **more frequent words contribute more**\n",
    "\n",
    "We now have a complete score expression in probability form.  \n",
    "Before calculating it on a computer, we‚Äôll apply logarithms to avoid underflow.\n",
    "\n",
    "### Final Scoring Formula (with Logs)\n",
    "\n",
    "When classifying a new document, we compute:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{class} \\mid X) \\propto \\log P(\\text{class}) + \\sum_{i=1}^{n} x_i \\cdot \\log P(x_i \\mid \\text{class})\n",
    "$$\n",
    "\n",
    "Each word‚Äôs count $x_i$ acts like a **weight**, multiplying the impact of that word‚Äôs log-probability.\n",
    "\n",
    "The model picks the class with the highest final score.\n",
    "\n",
    "## Let's Code It\n",
    "Now that we understand how it works, let‚Äôs implement it from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc608d64-48fa-4926-a36d-56e631fa12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch accuracy = 0.6719\n"
     ]
    }
   ],
   "source": [
    "class MyMultinomialNB:\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        # Œ± = 1  ‚Üí classic Laplace smoothing (‚Äúadd-one‚Äù)\n",
    "        # prevents zero probabilities for unseen words\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # ==================== TRAIN ====================\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        X  ‚îÄ shape (n_docs, n_words)\n",
    "             Each value = how many times word i appears in the document\n",
    "        y  ‚îÄ shape (n_docs,)\n",
    "             Each value = class label (e.g. 0 = ham, 1 = spam)\n",
    "        \"\"\"\n",
    "        n_docs, n_words = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # ---------- 1. PRIOR P(class) ----------\n",
    "        class_counts = np.bincount(y, minlength=n_classes)\n",
    "        self.log_prior_ = np.log(class_counts / n_docs)  # shape (n_classes,)\n",
    "\n",
    "        # ---------- 2. WORD COUNTS ----------\n",
    "        # word_counts[c, j] = total number of times word j appears in class c\n",
    "        word_counts = np.zeros((n_classes, n_words))\n",
    "\n",
    "        for c in self.classes_:\n",
    "            X_class = X[y == c]\n",
    "            word_counts[c] = X_class.sum(axis=0)  # sum over documents in class\n",
    "\n",
    "        # total_words[c] = total number of all word tokens in class c\n",
    "        total_words = word_counts.sum(axis=1, keepdims=True)  # shape (n_classes, 1)\n",
    "\n",
    "        # ---------- 3. SMOOTHED WORD PROBABILITIES ----------\n",
    "        # Œ∏ = probability of word j given class c\n",
    "        # shape: (n_classes, n_words)\n",
    "        self.log_theta_ = np.log(\n",
    "            (word_counts + self.alpha) /\n",
    "            (total_words + self.alpha * n_words)\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    # =================== PREDICT ===================\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        For each document:\n",
    "        - Compute log‚Äëscore for each class:\n",
    "            log P(class) + sum_i x_i * log P(word_i | class)\n",
    "        - Choose class with highest score\n",
    "        \"\"\"\n",
    "        # X: shape (n_test_docs, n_words)\n",
    "        # log_theta_: shape (n_classes, n_words)\n",
    "\n",
    "        # Compute score matrix: (n_test_docs, n_classes)\n",
    "        scores = X @ self.log_theta_.T + self.log_prior_\n",
    "\n",
    "        return self.classes_[scores.argmax(axis=1)]\n",
    "\n",
    "my_nb = MyMultinomialNB(alpha=1.0).fit(X_train, y_train)\n",
    "y_pred_my = my_nb.predict(X_test)\n",
    "acc_my = accuracy_score(y_test, y_pred_my)\n",
    "print(f\"scratch accuracy = {acc_my:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543742e6-e2ba-44a4-8edb-fbcd0e943fb7",
   "metadata": {},
   "source": [
    "## It Works!\n",
    "\n",
    "The scratch model hits an accuracy of **0.6719**, matching `scikit-learn`.\n",
    "\n",
    "This confirms that the logic behind **Multinomial Naive Bayes** ‚Äî counting word frequencies, applying Laplace smoothing, computing log-scores, and picking the class with the highest total ‚Äî behaves exactly as expected.\n",
    "\n",
    "We‚Äôve successfully built **Multinomial Naive Bayes** from the ground up!"
   ]
  }
 ],
 "metadata": {
  "created": "2025-04-21T04:23:37.599707+00:00",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "updated": "2025-04-21T04:23:37.599707+00:00"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
